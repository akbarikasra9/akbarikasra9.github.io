[
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Note:\nThis website was created through the use of Quarto in Rstudio. Quarto is a good tool to use because it works well with R, and it makes it simple to publish a professional-looking website on GitHub.\nThe navigation bar is designed to be clear and easy to use. The “Home” tab gives a description about me and my demonstration making this website, and the “About” tab is a section that gives a quick introduction about me.\nAdditionally, I’ve uploaded my resume in the navigation bar, where I present my professional and academic background. This website is not only a place for this specific assignment, but also an opportunity for future use for other projects."
  },
  {
    "objectID": "resume2025.html",
    "href": "resume2025.html",
    "title": "Resume 2025",
    "section": "",
    "text": "Celina, TX, 75009 | (214)-499-8057 | akbarikasra9@gmail.com | LinkedIn\n\n\n\nThe University of Texas at Dallas — B.A. in Political Science\nGraduation: May 2024 | GPA: 3.6\nThe University of Texas at Dallas — M.S. in Social Data Analytics and Research\nExpected Graduation: May 2027\n\n\n\n\nDelegate, UTD Model United Nations — Jan 2023 – May 2024\n- Conducted market research on domestic and international policies, analyzing trends, institutions, and key stakeholders to develop policy solutions.\n- Compiled structured reports evaluating decision-making processes and policy effectiveness.\n- Delivered persuasive arguments in formal discussions, enhancing public speaking, analytical, and presentation skills.\nOfficer, UTD MOVE Texas — Sep 2023 – May 2024\n- Led strategic outreach initiatives to increase voter registration and civic engagement.\n- Organized workshops and data-driven engagement strategies to inform and mobilize communities.\n- Developed targeted marketing strategies to enhance student engagement.\nParticipant, John Marshall Pre-Law Society — Oct 2021 – May 2024\n- Gained expertise in business strategy and research methodologies through mentorship and networking.\n- Conducted policy research and legal analysis, refining critical thinking skills.\n- Developed competitive assessments to evaluate industry trends.\n\n\n\n\nStock Associate, Zara (Frisco, TX) — Dec 2024 – Present\n- Managing inventory operations with efficiency and accuracy in a fast-paced retail environment.\n- Utilizing data tracking systems to improve stock accuracy.\n- Maintaining store organization and product replenishment.\nSales Associate, TJ Maxx (Prosper, TX) — Dec 2022 – Dec 2024\n- Delivered excellent customer service and applied problem-solving skills to address customer concerns.\n- Assisted in merchandising strategies to optimize product visibility.\n- Processed transactions efficiently while maintaining accuracy.\n\n\n\n\n\nModel UN Position Paper Award — Feb 21, 2023 | Portland, OR\n\nModel UN Distinguished Delegation Award — Apr 6, 2023 | New York City, NY\n\nModel UN Outstanding Delegation Award — Nov 24, 2023 | Erfurt, Germany\n\nMath and Coding Camp Certificate of Completion — Aug 20, 2025 | Richardson, TX\n\n\n\n\n\n\nPublic Speaking & Presentation\n\nMicrosoft Office, Google Suite & CRM Tools\n\nQuantitative Research & Data Visualization (Excel)\n\nMultilingual: English, German, Persian\n\nData Analysis & Programming: R, SPSS, STATA"
  },
  {
    "objectID": "resume2025.html#education",
    "href": "resume2025.html#education",
    "title": "Resume 2025",
    "section": "",
    "text": "The University of Texas at Dallas — B.A. in Political Science\nGraduation: May 2024 | GPA: 3.6\nThe University of Texas at Dallas — M.S. in Social Data Analytics and Research\nExpected Graduation: May 2027"
  },
  {
    "objectID": "resume2025.html#professional-experience",
    "href": "resume2025.html#professional-experience",
    "title": "Resume 2025",
    "section": "",
    "text": "Delegate, UTD Model United Nations — Jan 2023 – May 2024\n- Conducted market research on domestic and international policies, analyzing trends, institutions, and key stakeholders to develop policy solutions.\n- Compiled structured reports evaluating decision-making processes and policy effectiveness.\n- Delivered persuasive arguments in formal discussions, enhancing public speaking, analytical, and presentation skills.\nOfficer, UTD MOVE Texas — Sep 2023 – May 2024\n- Led strategic outreach initiatives to increase voter registration and civic engagement.\n- Organized workshops and data-driven engagement strategies to inform and mobilize communities.\n- Developed targeted marketing strategies to enhance student engagement.\nParticipant, John Marshall Pre-Law Society — Oct 2021 – May 2024\n- Gained expertise in business strategy and research methodologies through mentorship and networking.\n- Conducted policy research and legal analysis, refining critical thinking skills.\n- Developed competitive assessments to evaluate industry trends."
  },
  {
    "objectID": "resume2025.html#work-experience",
    "href": "resume2025.html#work-experience",
    "title": "Resume 2025",
    "section": "",
    "text": "Stock Associate, Zara (Frisco, TX) — Dec 2024 – Present\n- Managing inventory operations with efficiency and accuracy in a fast-paced retail environment.\n- Utilizing data tracking systems to improve stock accuracy.\n- Maintaining store organization and product replenishment.\nSales Associate, TJ Maxx (Prosper, TX) — Dec 2022 – Dec 2024\n- Delivered excellent customer service and applied problem-solving skills to address customer concerns.\n- Assisted in merchandising strategies to optimize product visibility.\n- Processed transactions efficiently while maintaining accuracy."
  },
  {
    "objectID": "resume2025.html#achievements",
    "href": "resume2025.html#achievements",
    "title": "Resume 2025",
    "section": "",
    "text": "Model UN Position Paper Award — Feb 21, 2023 | Portland, OR\n\nModel UN Distinguished Delegation Award — Apr 6, 2023 | New York City, NY\n\nModel UN Outstanding Delegation Award — Nov 24, 2023 | Erfurt, Germany\n\nMath and Coding Camp Certificate of Completion — Aug 20, 2025 | Richardson, TX"
  },
  {
    "objectID": "resume2025.html#skills",
    "href": "resume2025.html#skills",
    "title": "Resume 2025",
    "section": "",
    "text": "Public Speaking & Presentation\n\nMicrosoft Office, Google Suite & CRM Tools\n\nQuantitative Research & Data Visualization (Excel)\n\nMultilingual: English, German, Persian\n\nData Analysis & Programming: R, SPSS, STATA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About Kasra Akbari\nKasra Akbari is a Social Data Analytics Major at UT Dallas.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kasra Akbari",
    "section": "",
    "text": "Welcome to the world of data science.\nMy name is Kasra Akbari, and I am a Social Data Analytics major at the University of Texas at Dallas and I made this website to demonstrate my skills using R and Quarto.\n\nplot(iris, pch=20, cex= .8, col=\"firebrick\")"
  },
  {
    "objectID": "podcast.html",
    "href": "podcast.html",
    "title": "Podcast Review",
    "section": "",
    "text": "Karen Ng is the Head of Product for a company called HubSpot, and in this podcast, she discusses how data is the currency that AI is running and will continue to run in the future, where there will be a world where AI will grow faster and stronger. However, humans still play an important role in data gathering.\n\n\n\nOne point I find interesting is how Ms. Ng mentions the future looking to be hybrid for people’s work with data. In a hybrid setting, there are three important roles.\nFirst are chatbots, which are useful for answering questions quickly. Secondly, copilots are human assistants, and lastly there are data agents, where their role is to help clean data and help build a context layer.\nMs. Ng discusses the agents’ essential role in a team, where making an agent platform makes everything easier because they provide instructions and information on how to gather and clean data.\n\n\n\n\nAn additional point I found interesting in the podcast are interoffice dynamics because it demonstrates job operations in a hybrid-office team. There are three blueprints on a hybrid-office team.\nThe first blueprint is context-sharing. Where does data come from? How can it be tackled? What solutions come out at the end of the process?\nThe second part is building customer-relations renewals and policies.\nFinally, building the actual team: how do we get the resources? How do we build the team once the tools are received? It has been proven that finding a shared language for data use makes it easy to help and measure customer-relations and its values.\n\n\n\n\nOverall, this podcast has made great points on the functions of a hybrid-team with their uses of AI. It makes many interesting points and raises important questions on the long-term effects of data collection and the future of the use of AI in the data-driven world."
  },
  {
    "objectID": "podcast.html#hybrid-roles-in-data-teams",
    "href": "podcast.html#hybrid-roles-in-data-teams",
    "title": "Podcast Review",
    "section": "",
    "text": "One point I find interesting is how Ms. Ng mentions the future looking to be hybrid for people’s work with data. In a hybrid setting, there are three important roles.\nFirst are chatbots, which are useful for answering questions quickly. Secondly, copilots are human assistants, and lastly there are data agents, where their role is to help clean data and help build a context layer.\nMs. Ng discusses the agents’ essential role in a team, where making an agent platform makes everything easier because they provide instructions and information on how to gather and clean data."
  },
  {
    "objectID": "podcast.html#interoffice-dynamics-in-hybrid-teams",
    "href": "podcast.html#interoffice-dynamics-in-hybrid-teams",
    "title": "Podcast Review",
    "section": "",
    "text": "An additional point I found interesting in the podcast are interoffice dynamics because it demonstrates job operations in a hybrid-office team. There are three blueprints on a hybrid-office team.\nThe first blueprint is context-sharing. Where does data come from? How can it be tackled? What solutions come out at the end of the process?\nThe second part is building customer-relations renewals and policies.\nFinally, building the actual team: how do we get the resources? How do we build the team once the tools are received? It has been proven that finding a shared language for data use makes it easy to help and measure customer-relations and its values."
  },
  {
    "objectID": "podcast.html#final-thoughts",
    "href": "podcast.html#final-thoughts",
    "title": "Podcast Review",
    "section": "",
    "text": "Overall, this podcast has made great points on the functions of a hybrid-team with their uses of AI. It makes many interesting points and raises important questions on the long-term effects of data collection and the future of the use of AI in the data-driven world."
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "#install.packages(\"gtrendsR\")\nlibrary(gtrendsR)\n\nTrumpHarrisElection = gtrends(c(\"Trump\",\"Harris\",\"election\"),\n                              onlyInterest = TRUE, geo = \"US\",\n                              gprop = \"web\", time = \"today+5-y\",\n                              category = 0)\nthe_df = TrumpHarrisElection$interest_over_time\nplot(TrumpHarrisElection)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the gtrendsR package.\n  Please report the issue at &lt;https://github.com/PMassicotte/gtrendsR/issues&gt;.\n\n\n\n\n\n\n\n\ntg = gtrends(\"tariff\", time = \"all\")\n\nWhen it comes to using and gathering data from the Google Trends website and the gtrendsR Package, they differ in multiple ways, but they also share similarity of data visualization. I gathered my data from Google Trends on Trump, Kamala Harris and the elections over the last five years, and used the gtrendsR01.R package to code and provide the data about those topics, as shown on the visual above. The dates, intervals, and graphs from both methods appeared similar, demonstrating spikes of the 2020 and 2024 elections.\nThe Google Trends method is easy to use because individuals can categorize one column per keyword, and the date coverage can be set manually however they like. One minor setback of this method is that the data must be downloaded manually, but it’s still manageable.\nOn the contrary, the gtrendsR package is a method that requires more work and can be complex, as it involves the use of R codes for automatic data downloads, which also requires coding knowledge. However, the repetitions are simple and can be easily scripted unlike the website, making it more efficient and reproducible once it is set up. It’s also easy to code and create a CSV file through the use of R."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "#install.packages(c(\"tidycensus\", \"tigris\", \"sf\", \"dplyr\",\"ggplot2\", \"readr\", \"tidyr\"))\nlibrary(tidycensus)\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# 1) API key (example)\n# census_api_key(\"c5c7d8f0315d0bb7a67c1a7549772a162a4eecfa\", install = FALSE)\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n# View a few example codes\nvars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n# 3) Parameters (EDIT ME)\nstate_abbr &lt;- \"CA\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n# 4) Download\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2019-2023 5-year ACS\n\n# 5) Wide format for convenience\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n# 6) Map (edit titles/theme)\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = NA) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 7) Table (top/bottom by poverty count)\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_poverty)) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_poverty) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.3423 ymin: 32.53444 xmax: -114.1312 ymax: 38.7364\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Los Angeles County, C…          1322476       15552 (((-118.6044 33.47855, -…\n 2 San Diego County, Cal…           330602        7963 (((-117.596 33.38779, -1…\n 3 Orange County, Califo…           296493        8509 (((-118.1146 33.74461, -…\n 4 San Bernardino County…           291226        9076 (((-117.8025 33.97555, -…\n 5 Riverside County, Cal…           266955        8729 (((-117.6763 33.88882, -…\n 6 Sacramento County, Ca…           197472        6775 (((-121.8625 38.06795, -…\n 7 Fresno County, Califo…           185717        5965 (((-120.9094 36.7477, -1…\n 8 Kern County, Californ…           168825        6993 (((-120.1944 35.78936, -…\n 9 Alameda County, Calif…           149752        4801 (((-122.3423 37.80556, -…\n10 Santa Clara County, C…           128470        5622 (((-122.2027 37.36305, -…\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.256 ymin: 35.78669 xmax: -115.648 ymax: 42.00076\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Alpine County, Califo…              209          98 (((-120.0724 38.70277, -…\n 2 Sierra County, Califo…              325         202 (((-121.0575 39.53999, -…\n 3 Mono County, Californ…             1441         480 (((-119.6489 38.28912, -…\n 4 Modoc County, Califor…             1717         357 (((-121.4572 41.94994, -…\n 5 Inyo County, Californ…             1928         439 (((-118.79 37.39403, -11…\n 6 Plumas County, Califo…             2075         538 (((-121.497 40.43702, -1…\n 7 Colusa County, Califo…             2332         598 (((-122.7851 39.38298, -…\n 8 Mariposa County, Cali…             2347         449 (((-120.3944 37.67504, -…\n 9 Trinity County, Calif…             2830         755 (((-123.6224 40.9317, -1…\n10 Del Norte County, Cal…             2900         589 (((-124.2175 41.95081, -…\n\n# 8) Save outputs (optional)\n# readr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")"
  },
  {
    "objectID": "assign04.html",
    "href": "assign04.html",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "# Load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n# Import dataset\nfores &lt;- read_csv(\"data/fores.csv\", col_types = cols(NA...7 = col_skip(), \n    NA...8 = col_skip(), newdate = col_skip()))\n\nNew names:\n• `NA` -&gt; `NA...7`\n• `NA` -&gt; `NA...8`\n\nView(fores)\nforeign_reserves_data &lt;- read_csv(\"data/foreign_reserves_data.csv\", \n    col_types = cols(Other_Currencies = col_skip(), \n        Unallocated_Reserves = col_skip(), \n        `NA` = col_skip()))\nView(foreign_reserves_data)\n\n# Top 5 rows for each data frame\nhead(fores, 5)\n\n# A tibble: 5 × 6\n  Rank                               Country   Forexres     Date  Change Sources\n  &lt;chr&gt;                              &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 Country(as recognized by the U.N.) Continent Including g… Incl… Exclu… Exclud…\n2 Country(as recognized by the U.N.) Continent millions U.… Chan… milli… Change \n3 China                              Asia      3,643,149    41,0… 3,389… 31,221 \n4 Japan                              Asia      1,324,210    19,7… 1,230… 16,230 \n5 Switzerland                        Europe    1,007,710    13,9… 897,2… 14,490 \n\nhead(foreign_reserves_data, 5)\n\n# A tibble: 5 × 11\n   Year Quarter USD      Euro     JPY    GBP    CAN    CNY    AUD    CHF   Total\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n1    NA &lt;NA&gt;    USD      EUR      JPY    GBP    CAD    CNY    AUD    CHF   Total\n2  2019 Q1      6,727.09 2,208.79 584.63 495.70 208.64 212.26 181.95 15.27 11,6…\n3  2019 Q2      6,752.28 2,264.88 611.87 497.41 209.85 212.80 186.71 15.53 11,7…\n4  2019 Q3      6,728.85 2,212.74 612.75 492.22 205.44 213.83 182.48 16.20 11,6…\n5  2019 Q4      6,674.83 2,279.30 631.00 511.51 206.71 215.81 187.18 17.36 11,8…\n\n\nTo gain web data for research, the R programming language and its rvest01 package is an efficient approach to data collection, production and web scraping. Using a website’s developer tools to copy any Xpaths helps the researcher target and retrieve specific information. However, it is important to know that not every website allows web scraping due to privacy and ethical concerns, so it’s best to scrape any public data from public websites as much as possible. When scraping from a website, researchers should identify essential XPaths, integrate them into an R script, and deploy the rvest01 package to gain a structured data from the website. It’s important to remove any unnecessary information to make the data look more presentable. These methods are efficient for web-based data collection and preparations for any research analysis.\nTo demonstrate this process, I apply my method in this assignment by retrieving data from Wikipedia on foreign exchange reserves. I identified two tables of interest, which are “Foreign Exchange Reserves By Country” and “Currency Composition Of Foreign Exchange Reserves” (COFER). The XPaths corresponding to each table were extracted and applied within R to obtain the datasets. To make the data look more presentable, I removed unneccessary data. This created an efficient organization and preparation process of data from a public source, showing the practical use of the rvest01 package in managing web-based data acquisition."
  },
  {
    "objectID": "assign05.html",
    "href": "assign05.html",
    "title": "Assignment 5: Government Data API",
    "section": "",
    "text": "For this assignment, I used the gov01data.R package to scrape government documents. Two issues have been encountered, but they have been solved. Here is an overview of the scraping process:\n\nFile Format Requirements: The initial challenges involved downloading the dataset listings. I assumed that only the CSV file was required; however, the govdata01 package relies on both files, which in this case are the CSV and JSON files. This was a necessary step because it ensures compatibility with the package’s functions and to enable proper data extraction.\nFile Downloads: During the download process, the last five entries failed to retrieve successfully due to missing or empty pdf link fields in the JSON data. Items 6-10 were shown as “Failed to download” because those records did not have valid PDF URLs.\nData Usability and Recommendations: Despite these challenges, the resulting dataset remains usable, as it includes a diverse range of government documents from recent times. For future development, implementing a stronger filtering mechanism—such as automatically excluding records with missing pdf link fields—would improve the overall efficiency of the document retrieval process. Below, I’ve pasted my codes and the results.\n\n\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n\ngc(reset=T)\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  603827 32.3    1374185 73.4   603827 32.3\nVcells 1096251  8.4    8388608 64.0  1096251  8.4\n\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\"\\\\directory\\\\subdirectory\\\\\n## For Mac, \"/Users/YOURNAME/path/\"\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"data/govinfo-search-results-2025-10-23T01_43_22.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list &lt;- rjson::fromJSON(file =\"data/govinfo-search-results-2025-10-23T01_43_50.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"data/govinfo-search-results-2025-10-23T01_43_50.json\")\n\n### Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n### One more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles3$pdfLink\npdf_govfiles_id &lt;- govfiles3$index\n\n# Directory to save the pdf's\n# Be sure to create a folder for storing the pdf's\nsave_dir &lt;- \"datamethods\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\n\n## Try ten\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL '': status\nwas 'URL using bad/illegal format or missing URL'\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL '': status\nwas 'URL using bad/illegal format or missing URL'\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL '': status\nwas 'URL using bad/illegal format or missing URL'\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL '': status\nwas 'URL using bad/illegal format or missing URL'\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL '': status\nwas 'URL using bad/illegal format or missing URL'\n\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 11.09595 secs\n\n# Print results\nprint(results)\n\n [1] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CCAL-119hcal-2025-10-21/pdf/CCAL-119hcal-2025-10-21-pt9.pdf\" \n [2] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CCAL-119hcal-2025-10-21/pdf/CCAL-119hcal-2025-10-21-pt13.pdf\"\n [3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CCAL-119hcal-2025-10-21/pdf/CCAL-119hcal-2025-10-21-pt22.pdf\"\n [4] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CCAL-119scal-2025-10-21/pdf/CCAL-119scal-2025-10-21-pt2.pdf\" \n [5] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CCAL-119scal-2025-10-21/pdf/CCAL-119scal-2025-10-21-pt6.pdf\" \n [6] \"Failed to download: \"                                                                                                     \n [7] \"Failed to download: \"                                                                                                     \n [8] \"Failed to download: \"                                                                                                     \n [9] \"Failed to download: \"                                                                                                     \n[10] \"Failed to download: \"                                                                                                     \n\n## Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?"
  },
  {
    "objectID": "assign05.html#website-govinfo-httpswww.govinfo.govappsearch",
    "href": "assign05.html#website-govinfo-httpswww.govinfo.govappsearch",
    "title": "Assignment 5: Government Data API",
    "section": "Website: GovInfo (https://www.govinfo.gov/app/search/)",
    "text": "Website: GovInfo (https://www.govinfo.gov/app/search/)"
  },
  {
    "objectID": "assign05.html#prerequisite-download-from-website-the-list-of-files-to-be-downloaded",
    "href": "assign05.html#prerequisite-download-from-website-the-list-of-files-to-be-downloaded",
    "title": "Assignment 5: Government Data API",
    "section": "Prerequisite: Download from website the list of files to be downloaded",
    "text": "Prerequisite: Download from website the list of files to be downloaded"
  },
  {
    "objectID": "assign05.html#designed-for-background-job",
    "href": "assign05.html#designed-for-background-job",
    "title": "Assignment 5: Government Data API",
    "section": "Designed for background job",
    "text": "Designed for background job"
  },
  {
    "objectID": "assign05.html#set-path-for-reading-the-listing-and-home-directory",
    "href": "assign05.html#set-path-for-reading-the-listing-and-home-directory",
    "title": "Assignment 5: Government Data API",
    "section": "Set path for reading the listing and home directory",
    "text": "Set path for reading the listing and home directory"
  },
  {
    "objectID": "assign05.html#for-windows-use-cdirectorysubdirectory",
    "href": "assign05.html#for-windows-use-cdirectorysubdirectory",
    "title": "Assignment 5: Government Data API",
    "section": "For Windows, use “c:”\\directory\\subdirectory\\",
    "text": "For Windows, use “c:”\\directory\\subdirectory\\"
  },
  {
    "objectID": "assign05.html#for-mac-usersyournamepath",
    "href": "assign05.html#for-mac-usersyournamepath",
    "title": "Assignment 5: Government Data API",
    "section": "For Mac, “/Users/YOURNAME/path/”",
    "text": "For Mac, “/Users/YOURNAME/path/”\nlibrary(rjson) library(jsonlite) library(data.table) library(readr)"
  },
  {
    "objectID": "assign05.html#csv-method",
    "href": "assign05.html#csv-method",
    "title": "Assignment 5: Government Data API",
    "section": "CSV method",
    "text": "CSV method\ngovfiles= read.csv(file=“govinfo-search-results-2025-10-22T02_04_09.csv”, skip=2)"
  },
  {
    "objectID": "assign05.html#json-method",
    "href": "assign05.html#json-method",
    "title": "Assignment 5: Government Data API",
    "section": "JSON method",
    "text": "JSON method\n\nrjson\ngf_list &lt;- rjson::fromJSON(file =“govinfo-search-results-2025-10-22T02_05_19.json”) govfile2=dplyr::bind_rows(gf_list$resultSet)\n\n\njsonlite\ngf_list1 = jsonlite::read_json(“govinfo-search-results-2025-10-22T02_05_19.json”)\n\n\nExtract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n\nOne more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()"
  },
  {
    "objectID": "assign05.html#try-downloading-one-document",
    "href": "assign05.html#try-downloading-one-document",
    "title": "Assignment 5: Government Data API",
    "section": "Try downloading one document",
    "text": "Try downloading one document\nstart.time &lt;- Sys.time() message(“Starting downloads”) results &lt;- 1:1 %&gt;% purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.])) message(“Finished downloads”) end.time &lt;- Sys.time() time.taken &lt;- end.time - start.time time.taken"
  },
  {
    "objectID": "assign05.html#try-five",
    "href": "assign05.html#try-five",
    "title": "Assignment 5: Government Data API",
    "section": "Try five",
    "text": "Try five\nstart.time &lt;- Sys.time() message(“Starting downloads”) results &lt;- 1:5 %&gt;% purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.])) message(“Finished downloads”) end.time &lt;- Sys.time() time.taken &lt;- end.time - start.time time.taken"
  },
  {
    "objectID": "assign05.html#download-all-caution-this-may-take-a-while-and-lots-of-space",
    "href": "assign05.html#download-all-caution-this-may-take-a-while-and-lots-of-space",
    "title": "Assignment 5: Government Data API",
    "section": "Download all: Caution, this may take a while and lots of space",
    "text": "Download all: Caution, this may take a while and lots of space\nstart.time &lt;- Sys.time() message(“Starting downloads”) results &lt;- 1:length(pdf_govfiles_url) %&gt;% purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.])) message(“Finished downloads”) end.time &lt;- Sys.time() time.taken &lt;- end.time - start.time time.taken"
  },
  {
    "objectID": "assign05.html#exercise-try-downloading-118th-congress-congressional-hearings-in-committee-on-foreign-affairs",
    "href": "assign05.html#exercise-try-downloading-118th-congress-congressional-hearings-in-committee-on-foreign-affairs",
    "title": "Assignment 5: Government Data API",
    "section": "Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?",
    "text": "Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?"
  }
]